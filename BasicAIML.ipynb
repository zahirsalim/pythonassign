{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Applied Stats Assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Emp#</th>\n",
       "      <th>YearsOfExp</th>\n",
       "      <th>Salary in Rs.</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>126015.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>12598.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>100639.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>125351.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>25031.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Emp#  YearsOfExp  Salary in Rs.\n",
       "0   1.0        10.0       126015.0\n",
       "1   2.0         1.0        12598.0\n",
       "2   3.0         8.0       100639.0\n",
       "3   4.0        10.0       125351.0\n",
       "4   5.0         2.0        25031.0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats = pd.read_excel(\"Stats.xlsx\")\n",
    "stats.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Salary 77821.2\n",
      "Mean Years of exp 6.199999999999999\n",
      "Median Salary 87654.0\n",
      "Median Years of exp 6.6\n",
      "Mode Salary 100639.0\n",
      "Mode Years of exp 10.0\n"
     ]
    }
   ],
   "source": [
    "#Question 1 - Mean , Mode and Median of Years / Salary\n",
    "mean_salary = stats[\"Salary in Rs.\"].mean()\n",
    "print(f\"Mean Salary {mean_salary}\")\n",
    "mean_years = stats[\"YearsOfExp\"].mean()\n",
    "print(f\"Mean Years of exp {mean_years}\")\n",
    "median_salary = stats[\"Salary in Rs.\"].median()\n",
    "print(f\"Median Salary {median_salary}\")\n",
    "median_years = stats[\"YearsOfExp\"].median()\n",
    "print(f\"Median Years of exp {median_years}\")\n",
    "mode_salary = stats[\"Salary in Rs.\"].mode().iloc[0]\n",
    "print(f\"Mode Salary {mode_salary}\")\n",
    "mode_years = stats[\"YearsOfExp\"].mode().iloc[0]\n",
    "print(f\"Mode Years of exp {mode_years}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Salary Standard Deviation 39847.61528100939\n",
      "Years Standard Deviation 3.111269837220809\n",
      "Salary Variance 1587832443.5833333\n",
      "Years Variance 9.68\n"
     ]
    }
   ],
   "source": [
    "#Question 2 - Standard Deviation and Variance\n",
    "std_salary = stats[\"Salary in Rs.\"].std()\n",
    "std_years = stats[\"YearsOfExp\"].std()\n",
    "var_salary = stats[\"Salary in Rs.\"].var()\n",
    "var_years = stats[\"YearsOfExp\"].var()\n",
    "print(f\"Salary Standard Deviation {std_salary}\")\n",
    "print(f\"Years Standard Deviation {std_years}\")\n",
    "print(f\"Salary Variance {var_salary}\")\n",
    "print(f\"Years Variance {var_years}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NLP Assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (GPE Hello/NNP)\n",
      "  there/RB\n",
      "  ,/,\n",
      "  how/WRB\n",
      "  are/VBP\n",
      "  you/PRP\n",
      "  ?/.\n",
      "  Weather/''\n",
      "  is/VBZ\n",
      "  awesome/JJ\n",
      "  ./.\n",
      "  Its/PRP$\n",
      "  raining/VBG\n",
      "  here/RB\n",
      "  now/RB\n",
      "  ./.)\n",
      "(S\n",
      "  (PERSON Hello/NNP)\n",
      "  (PERSON Mr./NNP Raja/NNP)\n",
      "  ,/,\n",
      "  how/WRB\n",
      "  are/VBP\n",
      "  you/PRP\n",
      "  ?/.\n",
      "  Weather/''\n",
      "  is/VBZ\n",
      "  awesome/JJ\n",
      "  ./.\n",
      "  Its/PRP$\n",
      "  raining/VBG\n",
      "  here/RB\n",
      "  now/RB\n",
      "  ./.)\n",
      "(S\n",
      "  (PERSON Hello/NNP)\n",
      "  (PERSON Mr./NNP Raja/NNP)\n",
      "  ,/,\n",
      "  how/WRB\n",
      "  are/VBP\n",
      "  you/PRP\n",
      "  ./.\n",
      "  Weather/CC\n",
      "  is/VBZ\n",
      "  bad/JJ\n",
      "  ./.\n",
      "  Its/PRP$\n",
      "  heavily/RB\n",
      "  raining/VBG\n",
      "  here/RB\n",
      "  now/RB\n",
      "  ./.)\n",
      "(S\n",
      "  (ORGANIZATION NLP/NNP)\n",
      "  is/VBZ\n",
      "  great/JJ\n",
      "  technique/NN\n",
      "  ./.\n",
      "  It/PRP\n",
      "  is/VBZ\n",
      "  nice/JJ\n",
      "  to/TO\n",
      "  learn/VB\n",
      "  this/DT\n",
      "  technique/NN\n",
      "  ./.)\n",
      "(S\n",
      "  AI/NNP\n",
      "  is/VBZ\n",
      "  making/VBG\n",
      "  difference/NN\n",
      "  in/IN\n",
      "  this/DT\n",
      "  world/NN\n",
      "  now/RB\n",
      "  ./.\n",
      "  It/PRP\n",
      "  would/MD\n",
      "  be/VB\n",
      "  helpful/JJ\n",
      "  for/IN\n",
      "  betterment/NN\n",
      "  of/IN\n",
      "  human/JJ\n",
      "  life/NN\n",
      "  ./.\n",
      "  We/PRP\n",
      "  need/VBP\n",
      "  to/TO\n",
      "  make/VB\n",
      "  advantage/NN\n",
      "  of/IN\n",
      "  that/DT\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "#Question 1\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.chunk import ne_chunk\n",
    "data_in = pd.read_excel(\"data_in.xlsx\")\n",
    "#Extract tokens from each comment\n",
    "comments = data_in[\"Comment\"].str.replace('\"','')\n",
    "tokens = comments.apply(sent_tokenize)\n",
    "word_tokens = comments.apply(word_tokenize)\n",
    "word_list = word_tokens.tolist()\n",
    "for line in word_list:\n",
    "    chunk_sent = ne_chunk(pos_tag(line))\n",
    "    print(chunk_sent)\n",
    "\n",
    "tokens.to_csv(\"data_out_sent.csv\",header=False,index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Question 2\n",
    "from nltk.tokenize import word_tokenize\n",
    "data_in = pd.read_excel(\"data_in.xlsx\")\n",
    "\n",
    "#Extract tokens from each comment\n",
    "comments = data_in[\"Comment\"].str.replace('\"','')\n",
    "tokens = comments.apply(word_tokenize)\n",
    "tokens.to_csv(\"data_out_word.csv\",header=False,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Hello', 'NNP'), ('there', 'RB'), (',', ','), ('how', 'WRB'), ('are', 'VBP'), ('you', 'PRP'), ('?', '.'), ('Weather', \"''\"), ('is', 'VBZ'), ('awesome', 'JJ'), ('.', '.'), ('Its', 'PRP$'), ('raining', 'VBG'), ('here', 'RB'), ('now', 'RB'), ('.', '.')]\n",
      "[('Hello', 'NNP'), ('Mr.', 'NNP'), ('Raja', 'NNP'), (',', ','), ('how', 'WRB'), ('are', 'VBP'), ('you', 'PRP'), ('?', '.'), ('Weather', \"''\"), ('is', 'VBZ'), ('awesome', 'JJ'), ('.', '.'), ('Its', 'PRP$'), ('raining', 'VBG'), ('here', 'RB'), ('now', 'RB'), ('.', '.')]\n",
      "[('Hello', 'NNP'), ('Mr.', 'NNP'), ('Raja', 'NNP'), (',', ','), ('how', 'WRB'), ('are', 'VBP'), ('you', 'PRP'), ('.', '.'), ('Weather', 'CC'), ('is', 'VBZ'), ('bad', 'JJ'), ('.', '.'), ('Its', 'PRP$'), ('heavily', 'RB'), ('raining', 'VBG'), ('here', 'RB'), ('now', 'RB'), ('.', '.')]\n",
      "[('NLP', 'NNP'), ('is', 'VBZ'), ('great', 'JJ'), ('technique', 'NN'), ('.', '.'), ('It', 'PRP'), ('is', 'VBZ'), ('nice', 'JJ'), ('to', 'TO'), ('learn', 'VB'), ('this', 'DT'), ('technique', 'NN'), ('.', '.')]\n",
      "[('AI', 'NNP'), ('is', 'VBZ'), ('making', 'VBG'), ('difference', 'NN'), ('in', 'IN'), ('this', 'DT'), ('world', 'NN'), ('now', 'RB'), ('.', '.'), ('It', 'PRP'), ('would', 'MD'), ('be', 'VB'), ('helpful', 'JJ'), ('for', 'IN'), ('betterment', 'NN'), ('of', 'IN'), ('human', 'JJ'), ('life', 'NN'), ('.', '.'), ('We', 'PRP'), ('need', 'VBP'), ('to', 'TO'), ('make', 'VB'), ('advantage', 'NN'), ('of', 'IN'), ('that', 'DT'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "#Question 3\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.tokenize import word_tokenize\n",
    "with open(\"NLPdataEx3&4-data_in.txt\", \"r\") as f:\n",
    "    lines = f.readlines()\n",
    "    for line in lines:\n",
    "        tokens = word_tokenize(line)\n",
    "        print(pos_tag(tokens))\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Sentence\n",
      "Hello there, how are you? Weather is awesome. Its raining here now.\n",
      "\n",
      "Lemmatized Tokens\n",
      "['Hello', ',', '?', 'Weather', 'awesome', '.', 'Its', 'raining', '.']\n",
      "Stemmed Tokens\n",
      "['hello', ',', '?', 'weather', 'awesom', '.', 'it', 'rain', '.']\n",
      "Original Sentence\n",
      "Hello Mr. Raja, how are you? Weather is awesome. Its raining here now.\n",
      "\n",
      "Lemmatized Tokens\n",
      "['Hello', 'Mr.', 'Raja', ',', '?', 'Weather', 'awesome', '.', 'Its', 'raining', '.']\n",
      "Stemmed Tokens\n",
      "['hello', 'mr.', 'raja', ',', '?', 'weather', 'awesom', '.', 'it', 'rain', '.']\n",
      "Original Sentence\n",
      "Hello Mr. Raja, how are you. Weather is bad. Its heavily raining here now.\n",
      "\n",
      "Lemmatized Tokens\n",
      "['Hello', 'Mr.', 'Raja', ',', '.', 'Weather', 'bad', '.', 'Its', 'heavily', 'raining', '.']\n",
      "Stemmed Tokens\n",
      "['hello', 'mr.', 'raja', ',', '.', 'weather', 'bad', '.', 'it', 'heavili', 'rain', '.']\n",
      "Original Sentence\n",
      "NLP is great technique. It is nice to learn this technique.\n",
      "\n",
      "Lemmatized Tokens\n",
      "['NLP', 'great', 'technique', '.', 'It', 'nice', 'learn', 'technique', '.']\n",
      "Stemmed Tokens\n",
      "['nlp', 'great', 'techniqu', '.', 'It', 'nice', 'learn', 'techniqu', '.']\n",
      "Original Sentence\n",
      "AI is making difference in this world now.  It would be helpful for betterment of human life. We need to make advantage of that.\n",
      "\n",
      "Lemmatized Tokens\n",
      "['AI', 'making', 'difference', 'world', '.', 'It', 'would', 'helpful', 'betterment', 'human', 'life', '.', 'We', 'need', 'make', 'advantage', '.']\n",
      "Stemmed Tokens\n",
      "['AI', 'make', 'differ', 'world', '.', 'It', 'would', 'help', 'better', 'human', 'life', '.', 'We', 'need', 'make', 'advantag', '.']\n"
     ]
    }
   ],
   "source": [
    "#Question 4\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lem = WordNetLemmatizer()\n",
    "ps = PorterStemmer()\n",
    "with open(\"NLPdataEx3&4-data_in.txt\", \"r\") as f:\n",
    "    lines = f.readlines()\n",
    "    for line in lines:\n",
    "        print(\"Original Sentence\")\n",
    "        print(line)\n",
    "        tokens = word_tokenize(line)\n",
    "        filtered_tokens = [t for t in tokens if t not in stop_words]\n",
    "        lem_tokens = []\n",
    "        stem_tokens = []\n",
    "        for t in filtered_tokens:\n",
    "            lem_tokens.append(lem.lemmatize(t))\n",
    "            stem_tokens.append(ps.stem(t))\n",
    "        print(\"Lemmatized Tokens\")\n",
    "        print(lem_tokens)\n",
    "        print(\"Stemmed Tokens\")\n",
    "        print(stem_tokens)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rose is beautiful. : Positive\n",
      "Place is nasty to stay. : Negative\n",
      "This is the beauty of this technique. : Positive\n",
      "Concept is explained beautifully in this book. : Positive\n",
      "He annoyed me. : Negative\n",
      "Its the supreme place to stay. : Positive\n",
      "I hate this place. : Negative\n",
      "Dont annoy the customer. : Negative\n",
      "He has given nasty comments about his stay. : Negative\n",
      "Dessert is awesome. : Positive\n",
      "Your gift is wonderful. : Positive\n",
      "Overall Sentiment of text is Positive\n"
     ]
    }
   ],
   "source": [
    "#Question 5\n",
    "from nltk.tokenize import word_tokenize\n",
    "with open(\"NLPdataEx5dict.txt\",\"r\") as f:\n",
    "    lines = f.readlines()\n",
    "neg_words = lines[0].split(\"=\")[1].strip(\"'][\\n\").split(\",\")\n",
    "neg = [w.strip('\"') for w in neg_words]\n",
    "pos_words = lines[1].split(\"=\")[1].strip(\"'][\\n\").split(\",\")\n",
    "pos = [w.strip('\"') for w in pos_words]\n",
    "\n",
    "with open(\"NLPdataEx5data_senti_analyze.txt\",\"r\") as f:\n",
    "    lines = f.readlines()\n",
    "sentiment = lambda s : \"Positive\" if s > 0 else \"Negative\"\n",
    "overall_score = 0\n",
    "for line in lines:\n",
    "    score = 0\n",
    "    token = word_tokenize(line)\n",
    "    for t in token:\n",
    "        if t in neg:\n",
    "            score -= 1\n",
    "        elif t in pos:\n",
    "            score += 1\n",
    "    overall_score += score\n",
    "    line = line.strip(\"\\n\")        \n",
    "    print(f\"{line} : {sentiment(score)}\")\n",
    "\n",
    "print(f\"Overall Sentiment of text is {sentiment(overall_score)}\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
